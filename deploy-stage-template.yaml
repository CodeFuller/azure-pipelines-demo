parameters:
- name: envName
  type: string

- name: envDisplayName
  type: string

- name: appId
  type: string

stages:
- stage: DeployTo${{ parameters.envDisplayName }}
  displayName: Deploy to ${{ parameters.envDisplayName }}
  # Stage dependency must exist for correct reference of variables from other stage (see reference to stageDependencies.BuildStage below).
  # It can be omitted for neighbor stages (like in our case), because implicit dependency between stages exist.
  # However it will break if some intermediate stage is added. That is why we specify dependency explicitly.
  dependsOn: BuildStage
  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/master'))
  jobs:
  - deployment: DeployJob
    variables:
      ImportedTestVariable2: $[ stageDependencies.BuildStage.BuildJob.outputs['SetOutputVariables.TestVariable2'] ]
    displayName: Deploy to ${{ parameters.envDisplayName }}
    environment: ${{ parameters.envName }}
    strategy:
      runOnce:
        deploy:
          steps:
            - script: |
               # This is a sample usage of output variable from another stage / job.
               echo "ImportedTestVariable2 = '$(ImportedTestVariable2)'"
              displayName: Print Variables

            # There is no download artifacts task here because it is auto injected in the deploy hook - https://docs.microsoft.com/en-us/azure/devops/pipelines/process/deployment-jobs?view=azure-devops#descriptions-of-lifecycle-hooks
            # Just in case we check for artifacts existence.
            # S3 Upload task will not produce error in case of missing source files.
            - powershell: |
               $FilePath = "$(Pipeline.Workspace)/project-binaries/${{ parameters.appId }}-version.txt"
               if (-not (Test-Path $FilePath -PathType Leaf)) {
                   Write-Error "File $FilePath does not exist"
                   exit 1
               }
              displayName: Check Deploy Artifacts

            - task: S3Upload@1
              displayName: Deploy to S3
              inputs:
                awsCredentials: '$(AwsCredentials)'
                bucketName: '$(TargetBucketName)'
                sourceFolder: $(Pipeline.Workspace)/project-binaries
                targetFolder: ${{ parameters.envName }}
                globExpressions: ${{ parameters.appId }}-version.txt
                filesAcl: public-read
